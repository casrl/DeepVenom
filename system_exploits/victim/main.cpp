#include <emmintrin.h>
#include <iostream>
#include <iomanip>
#include <pthread.h>
#include <vector>
#include <unordered_map>
#include <tuple>
#include <iomanip>


#include <torch/torch.h>
// #include <torch/version.h>
#include <torch/script.h>
#include "cifar10.h"
#include "transform.h"
#include <typeinfo>

using transform::ConstantPad;
using transform::RandomCrop;
using transform::Resize;
using transform::RandomHorizontalFlip;

torch::jit::Module model;
std::string model_path;
std::string data_path;
std::string save_path;

std::map<std::string, std::tuple<uint64_t, uint64_t>> layer_map;

int main(int argc, char **argv) {
    int num_threads = 1;

    if (argc == 5) {
        save_path = argv[1];
        model_path = argv[2];
        data_path = argv[3];
        num_threads = std::stoi(argv[4]);
    } else if (argc == 4) {
        save_path = argv[1];
        model_path = argv[2];
        data_path = argv[3];
    } else if (argc == 3) {
        save_path = argv[1];
        model_path = argv[2];
        // Data
        std::cout << "Path of data: ";
        std::getline(std::cin, data_path);
    } else if (argc == 2) {
        save_path = argv[1];
        // Model
        std::cout << "Path of model: ";
        std::getline(std::cin, model_path);
        // Data
        std::cout << "Path of data: ";
        std::getline(std::cin, data_path);
    } else {
        // Save
        std::cout << "Path to save model: ";
        std::getline(std::cin, save_path);
        // Model
        std::cout << "Path of model: ";
        std::getline(std::cin, model_path);
        // Data
        std::cout << "Path of data: ";
        std::getline(std::cin, data_path);
    }

    std::cout << "Model: " << model_path << "\nData: " << data_path << "\nNumber of thread: " << num_threads << std::endl;
    std::cout << "Export location: " << save_path << std::endl;
    torch::set_num_threads(num_threads);

    // std::cout << "PyTorch C++ Version: " << TORCH_VERSION << std::endl;

    // [R] Define which device to use. i.e., CPU or CUDA 
    auto cuda_available = torch::cuda::is_available();
    torch::Device device(cuda_available ? torch::kCUDA : torch::kCPU);
    std::cout << (cuda_available ? "CUDA available. Training on GPU." : "Training on CPU.") << '\n';

    // [R] Hyper parameters
    const int64_t train_batch_size = 128;
    const int64_t test_batch_size = 100;
    const int64_t image_size = 32; // [R] TODO
    std::vector<double> img_mean = {0.4914, 0.4822, 0.4465};
    std::vector<double> img_std = {0.2023, 0.1994, 0.201};

    // Additional parameters
    const int64_t num_classes = 10; 
    const int64_t batch_size = 128;
    const size_t num_epochs = 3;
    const double learning_rate = 0.00002;
    const double weight_decay = 1e-5;

    const std::string CIFAR_data_path = data_path;

    std::cout << "Loading data..." << std::endl;
    // CIFAR10 custom dataset
    auto train_dataset = CIFAR10(CIFAR_data_path)
        .map(ConstantPad(4))
        .map(RandomHorizontalFlip())
        .map(RandomCrop({32, 32}))
        .map(torch::data::transforms::Normalize<>(img_mean, img_std))
        .map(torch::data::transforms::Stack<>());

    // Number of samples in the training set
    auto num_train_samples = train_dataset.size().value();

    std::cout << "Pre-processing data..." << std::endl;
    auto test_dataset = CIFAR10(CIFAR_data_path, CIFAR10::Mode::kTest)
        .map(Resize({32, 32}))
        .map(torch::data::transforms::Normalize<>(img_mean, img_std))
        .map(torch::data::transforms::Stack<>());

    // Number of samples in the testset
    auto num_test_samples = test_dataset.size().value();

    // Data loader
    printf("Creating data loader...\n");
    auto train_loader = torch::data::make_data_loader<torch::data::samplers::RandomSampler>(
        std::move(train_dataset), torch::data::DataLoaderOptions().batch_size(batch_size).drop_last(true));

    auto test_loader = torch::data::make_data_loader<torch::data::samplers::SequentialSampler>(
        std::move(test_dataset), torch::data::DataLoaderOptions().batch_size(batch_size).drop_last(true));

    // Load the model
    printf("Loading model...\n");
    torch::jit::Module model = torch::jit::load(model_path);
    std::cout << "Model loaded successfully!\n";
    model.train();

    // [R] Get the parameters of the model
    std::vector<torch::Tensor> parameters;
    for (const auto &tensor : model.parameters()) {
        if (tensor.requires_grad()) {
            parameters.push_back(tensor);
        }
    }
 
    // Optimizer
    torch::optim::Adam optimizer(parameters, torch::optim::AdamOptions(learning_rate).weight_decay(weight_decay)); 

    // Generate the map containing pointers of weights for each layer
    for (const auto& params : model.named_parameters()) {
        layer_map.insert({
            params.name,
            std::make_tuple(
                (uint64_t)params.value.data_ptr(),          // First value: data_ptr
                (uint64_t)params.value.nbytes()           // Second value: nbytes
            )
        });
    }


    // Print the layer map
    std::cout << "Printing layer map...\n";
    for (const auto& params : layer_map) {
        std::cout << params.first << "\t"
                << std::hex << std::get<0>(params.second) << "\t"
                << std::dec <<std::get<1>(params.second) << std::endl;
    }

    std::cout << "Fine-tuning...\n";


    // Train the model
    for (size_t epoch = 0; epoch != num_epochs; ++epoch) {
        // Initialize running metrics
        double running_loss = 0.0;
        size_t num_correct = 0;
        int batch_idx = 0;

        for (auto& batch : *train_loader) {
            auto data = batch.data;
            auto target = batch.target.squeeze();    
            std::vector<torch::jit::IValue> input;
            input.push_back(data);
            optimizer.zero_grad();
            // Forward pass
            auto output = model.forward(input).toTensor();
            // Calculate loss
            auto loss = torch::nn::functional::cross_entropy(output, target);
            loss.backward();
            optimizer.step();
            running_loss += loss.item<double>() * data.size(0);
            batch_idx += 1;
            // print batch no
            std::cout << "Batch: " << batch_idx << " Loss: " << loss.item<double>() << std::endl;
        }
        auto epoch_loss = running_loss / num_train_samples;
        std::cout << "Epoch: " << epoch << " Loss: " << epoch_loss << std::endl;
    }

    std::cout << "Finished Training" << std::endl;
    std::cout << "Saving model" << std::endl;
    model.save(save_path);

    return 0;
}
